{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/SJH/OneDrive - korea.ac.kr/문서/MBTI 500.csv'\n",
    "USER_SAMPLE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path, encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USER_SAMPLE :\n",
    "    data = data.groupby('type').sample(frac = 0.005)\n",
    "    data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['estp', 'estj', 'esfp', 'esfj', 'entp', 'entj', 'enfp', 'enfj', 'istp', 'istj', 'isfp', 'isfj', 'intp', 'intj', 'infp', 'infj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_words_list(word) :\n",
    "    synonymus = set()\n",
    "    for words in wordnet.synsets(word) :\n",
    "        for lemma in words.lemmas() :\n",
    "            x = lemma.name().replace(\"_\",\"\").replace(\"-\",\"\").lower()\n",
    "            x = \"\".join([char for char in x if char in 'abcdefghijklmnopqrstuvwxyz '])\n",
    "            if len(x) > 2 :\n",
    "                synonymus.add(x)\n",
    "    if word in synonymus :\n",
    "        synonymus.remove(word)\n",
    "    return synonymus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('conversation.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.synsets('conversation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement_words(sentence_split, num_of_words) :\n",
    "    cp = sentence_split.copy()\n",
    "    random.shuffle(cp)\n",
    "    change_num = 0\n",
    "    for wd in cp :\n",
    "        if len(wd) <=2 or wd in stopwords :\n",
    "            continue\n",
    "        si = list(new_words_list(wd))\n",
    "        if len(si) < 5 :\n",
    "            continue\n",
    "        change_word = random.choice(si)\n",
    "        cp = [change_word if wd == wds else wds for wds in cp]\n",
    "        change_num += 1\n",
    "        if change_num >= num_of_words :\n",
    "            break\n",
    "\n",
    "    return \" \".join(cp).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like love have hate brush ear ['hate', 'liewith', 'havegot', 'brush', 'like', 'ear']\n"
     ]
    }
   ],
   "source": [
    "tst = ['like', 'love', 'have', 'hate', 'brush', 'ear']\n",
    "sen = 'like love have hate brush ear'\n",
    "print(sen, replacement_words(sen.split(' '), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['type'].str.contains('N')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>way one way ask need bring food drink etc say ...</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>life experience dependant one chance mine exis...</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hope nice night future keep get bright know lo...</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>around help emotional trouble college hour awa...</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trailer game come get game footage thus rub mo...</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>water snake whatever call honestly get people ...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>aisle le twenty minute beat hey get super pass...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>total disuse lack understand se rather factor ...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>tab week still least tab last week even get ne...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>every class low level faus high chance hit low...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 posts  type\n",
       "0    way one way ask need bring food drink etc say ...  ENFJ\n",
       "1    life experience dependant one chance mine exis...  ENFJ\n",
       "2    hope nice night future keep get bright know lo...  ENFJ\n",
       "3    around help emotional trouble college hour awa...  ENFJ\n",
       "4    trailer game come get game footage thus rub mo...  ENFJ\n",
       "..                                                 ...   ...\n",
       "496  water snake whatever call honestly get people ...  INTP\n",
       "497  aisle le twenty minute beat hey get super pass...  INTP\n",
       "498  total disuse lack understand se rather factor ...  INTP\n",
       "499  tab week still least tab last week even get ne...  INTP\n",
       "500  every class low level faus high chance hit low...  INTP\n",
       "\n",
       "[486 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_sentences = data['posts'].tolist()\n",
    "nw_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_nums = 9 # How many generate per sentence?\n",
    "\n",
    "new_sentences = []\n",
    "alpha = 0.05\n",
    "\n",
    "for sens in nw_sentences :\n",
    "    wds = sens.split(' ')\n",
    "    # print(wds)\n",
    "    for i in range(augment_nums) :\n",
    "        v = int(alpha * 500)\n",
    "        k = replacement_words(wds, v)\n",
    "        # print(k)\n",
    "        new_sentences.append(' '.join(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4374 486\n"
     ]
    }
   ],
   "source": [
    "print(len(new_sentences), len(nw_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = pd.DataFrame(new_sentences, columns= ['posts'])\n",
    "ex['type']='N'\n",
    "ex.to_csv('Aug_N.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
